{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_section"
   },
   "source": [
    "# Module 6: Precipitation Frequency Analysis with NOAA Atlas 14\n",
    "## Design Storm Generation for Multiple AEP Events ‚òîüåßÔ∏è\n",
    "\n",
    "### Welcome to Design Storm Analysis!\n",
    "This module focuses on generating design storms for hydrologic and hydraulic modeling using NOAA Atlas 14 precipitation frequency data. You'll learn to automate the creation of multiple design storm hyetographs and generate professional summary tables for engineering reports.\n",
    "\n",
    "### What You'll Accomplish Today:\n",
    "‚úÖ Access NOAA Atlas 14 precipitation frequency data via API  \n",
    "‚úÖ Upload and apply custom temporal distributions (MSE 6 24-hour)  \n",
    "‚úÖ Generate design storms for multiple AEP events  \n",
    "‚úÖ Create professional hyetographs with proper formatting  \n",
    "‚úÖ Build summary tables for engineering reports  \n",
    "‚úÖ Export data for HEC-HMS and HEC-RAS models  \n",
    "‚úÖ Batch process multiple return periods efficiently  \n",
    "\n",
    "### Module Structure:\n",
    "1. **Mental Models** - Design storm concepts for H&H\n",
    "2. **NOAA Atlas 14** - Accessing precipitation frequency data\n",
    "3. **Temporal Distributions** - MSE 6 24-hour pattern\n",
    "4. **Design Storm Generation** - Creating hyetographs\n",
    "5. **Batch Processing** - Multiple AEP events\n",
    "6. **Summary Tables** - Professional reporting\n",
    "7. **Model Export** - HEC-HMS/RAS formats\n",
    "8. **Complete Workflow** - Production-ready automation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mental_models"
   },
   "source": [
    "## Part 1: Mental Models - Design Storms for H&H üß†\n",
    "\n",
    "### What is a Design Storm?\n",
    "\n",
    "A **design storm** is a synthetic rainfall event used for hydrologic/hydraulic modeling:\n",
    "- Based on **statistical analysis** of historical data\n",
    "- Represents a specific **return period** or **probability**\n",
    "- Used for **regulatory compliance** (FEMA, local standards)\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "| Term | Definition | Example |\n",
    "|------|------------|----------|\n",
    "| **ARI** | Annual Recurrence Interval | 100-year storm |\n",
    "| **AEP** | Annual Exceedance Probability | 1% (= 100-year) |\n",
    "| **Duration** | Total storm length | 24 hours |\n",
    "| **Depth** | Total precipitation | 6.5 inches |\n",
    "| **Hyetograph** | Rainfall vs time graph | Design storm time series |\n",
    "| **Temporal Distribution** | How rainfall is spread over time | MSE 6, SCS Type II |\n",
    "\n",
    "### AEP ‚Üî ARI Conversion\n",
    "\n",
    "```\n",
    "AEP (%) = 100 / ARI (years)\n",
    "ARI (years) = 100 / AEP (%)\n",
    "```\n",
    "\n",
    "| AEP | ARI | Common Use |\n",
    "|-----|-----|------------|\n",
    "| 50% | 2-year | Frequent event |\n",
    "| 10% | 10-year | Minor flooding |\n",
    "| 4% | 25-year | Moderate flooding |\n",
    "| 2% | 50-year | Major flooding |\n",
    "| 1% | 100-year | **Base flood (FEMA)** |\n",
    "| 0.5% | 200-year | Extreme event |\n",
    "| 0.2% | 500-year | Critical infrastructure |\n",
    "\n",
    "### The Design Storm Workflow\n",
    "\n",
    "```\n",
    "1. NOAA Atlas 14    ‚Üí  Get total precipitation depth for AEP/Duration\n",
    "2. Temporal Pattern ‚Üí  Apply distribution (MSE 6, SCS Type II, etc.)\n",
    "3. Hyetograph      ‚Üí  Create time series at desired interval\n",
    "4. Model Input     ‚Üí  Export to HEC-HMS/RAS format\n",
    "```\n",
    "\n",
    "### What Makes This Module Different\n",
    "\n",
    "Instead of creating **one storm at a time**, you'll learn to:\n",
    "- Generate **multiple AEP events** in one run\n",
    "- Create **comparative tables** for reports\n",
    "- **Batch export** for model input\n",
    "- Build **reusable workflows** for any project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Part 2: Setting Up Your Workspace üõ†Ô∏è\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "We'll use:\n",
    "- `pandas` - Time series and data manipulation\n",
    "- `numpy` - Numerical calculations\n",
    "- `matplotlib` - Hyetograph plotting\n",
    "- `urllib` - NOAA Atlas 14 API access (built-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"   Ready for precipitation frequency analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atlas14_api"
   },
   "source": [
    "## Part 3: NOAA Atlas 14 API Access üåê\n",
    "\n",
    "### What is NOAA Atlas 14?\n",
    "\n",
    "**NOAA Atlas 14** is the authoritative source for precipitation frequency estimates in the United States:\n",
    "- Published by NOAA's Hydrometeorological Design Studies Center (HDSC)\n",
    "- Based on statistical analysis of 50+ years of rainfall data\n",
    "- Covers all of CONUS (with regional volumes)\n",
    "- Updated periodically with new data\n",
    "\n",
    "### API Access Strategy\n",
    "\n",
    "We'll implement the API function based on production code from RAS Commander:\n",
    "1. Use `urllib` instead of `requests` for better compatibility\n",
    "2. Parse JavaScript-style response from NOAA\n",
    "3. Handle network errors gracefully\n",
    "4. Provide fallback for manual data entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atlas14_function"
   },
   "outputs": [],
   "source": [
    "def get_atlas14_data(latitude, longitude, units='english', timeout=30):\n",
    "    \"\"\"\n",
    "    Download NOAA Atlas 14 precipitation frequency data for a location.\n",
    "    \n",
    "    Based on RAS Commander implementation for production reliability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    latitude : float\n",
    "        Latitude in decimal degrees (positive for Northern Hemisphere)\n",
    "    longitude : float\n",
    "        Longitude in decimal degrees (negative for Western Hemisphere)\n",
    "    units : str\n",
    "        'english' for inches or 'metric' for millimeters\n",
    "    timeout : int\n",
    "        Request timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Precipitation depths (inches or mm)\n",
    "        Rows: Durations (5-min through 60-day)\n",
    "        Columns: Return periods (1-yr through 1000-yr)\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    ConnectionError : If unable to reach NOAA API\n",
    "    ValueError : If location is outside Atlas 14 coverage\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOAA HDSC API endpoint\n",
    "    base_url = \"https://hdsc.nws.noaa.gov/cgi-bin/hdsc/new/cgi_readH5.py\"\n",
    "    \n",
    "    # Build request parameters\n",
    "    params = {\n",
    "        'lat': latitude,\n",
    "        'lon': longitude,\n",
    "        'type': 'pf',  # Precipitation frequency\n",
    "        'data': 'depth',\n",
    "        'units': units,\n",
    "        'series': 'pds'  # Partial duration series\n",
    "    }\n",
    "    \n",
    "    # Build URL with query string\n",
    "    query_string = '&'.join(f\"{k}={v}\" for k, v in params.items())\n",
    "    url = f\"{base_url}?{query_string}\"\n",
    "    \n",
    "    print(f\"üì° Requesting Atlas 14 data for ({latitude:.4f}, {longitude:.4f})...\")\n",
    "    print(f\"   Units: {units}\")\n",
    "    \n",
    "    try:\n",
    "        # Create request with proper headers\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header('User-Agent', 'Python-urllib/3.0 (Educational)')\n",
    "        \n",
    "        # Make request\n",
    "        with urllib.request.urlopen(request, timeout=timeout) as response:\n",
    "            content = response.read().decode('utf-8')\n",
    "        \n",
    "        # Parse the response (JavaScript-style variable assignments)\n",
    "        data_dict = {}\n",
    "        for line in content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if '=' in line and not line.startswith('#'):\n",
    "                try:\n",
    "                    # Split on first '=' only\n",
    "                    var_name, value_str = line.split('=', 1)\n",
    "                    var_name = var_name.strip()\n",
    "                    value_str = value_str.strip()\n",
    "                    \n",
    "                    # Remove trailing semicolon\n",
    "                    if value_str.endswith(';'):\n",
    "                        value_str = value_str[:-1].strip()\n",
    "                    \n",
    "                    # Parse value safely\n",
    "                    try:\n",
    "                        value = ast.literal_eval(value_str)\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        value = value_str.strip('\"\\'')\n",
    "                    \n",
    "                    data_dict[var_name] = value\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        # Extract quantiles array (precipitation depths)\n",
    "        quantiles = data_dict.get('quantiles', [])\n",
    "        \n",
    "        if not quantiles:\n",
    "            raise ValueError(\"No precipitation data in API response\")\n",
    "        \n",
    "        # Standard durations and return periods\n",
    "        duration_labels = ['5-min', '10-min', '15-min', '30-min', '60-min', \n",
    "                          '2-hr', '3-hr', '6-hr', '12-hr', '24-hr', \n",
    "                          '2-day', '3-day', '4-day', '7-day', '10-day', \n",
    "                          '20-day', '30-day', '45-day', '60-day']\n",
    "        \n",
    "        ari_labels = ['1-yr', '2-yr', '5-yr', '10-yr', '25-yr', \n",
    "                     '50-yr', '100-yr', '200-yr', '500-yr', '1000-yr']\n",
    "        \n",
    "        # Build DataFrame\n",
    "        num_durations = min(len(quantiles), len(duration_labels))\n",
    "        num_aris = min(len(quantiles[0]) if quantiles else 0, len(ari_labels))\n",
    "        \n",
    "        precip_data = {}\n",
    "        for ari_idx in range(num_aris):\n",
    "            values = []\n",
    "            for dur_idx in range(num_durations):\n",
    "                val = quantiles[dur_idx][ari_idx] if ari_idx < len(quantiles[dur_idx]) else np.nan\n",
    "                # Convert to float if string\n",
    "                if isinstance(val, str):\n",
    "                    try:\n",
    "                        val = float(val)\n",
    "                    except (ValueError, TypeError):\n",
    "                        val = np.nan\n",
    "                values.append(val)\n",
    "            precip_data[ari_labels[ari_idx]] = values\n",
    "        \n",
    "        df = pd.DataFrame(precip_data, index=duration_labels[:num_durations])\n",
    "        \n",
    "        region = data_dict.get('region', 'Unknown')\n",
    "        print(f\"‚úÖ Downloaded Atlas 14 data successfully!\")\n",
    "        print(f\"   Region: {region}\")\n",
    "        print(f\"   Coverage: {num_durations} durations √ó {num_aris} return periods\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except urllib.error.URLError as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n‚ùå Connection error: {error_msg[:100]}\")\n",
    "        print(\"\\n   This may be due to:\")\n",
    "        print(\"   ‚Ä¢ Network restrictions (firewall, proxy, VPN)\")\n",
    "        print(\"   ‚Ä¢ NOAA server temporarily unavailable\")\n",
    "        print(\"   ‚Ä¢ Location outside Atlas 14 coverage\")\n",
    "        print(\"\\n   üìù Use manual data entry method (see next cell)\")\n",
    "        raise ConnectionError(f\"Cannot access NOAA API: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing Atlas 14 data: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Atlas 14 API function defined!\")\n",
    "print(\"   Production-ready implementation based on RAS Commander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_atlas14"
   },
   "source": [
    "### Get Atlas 14 Data for Your Location\n",
    "\n",
    "**Try the API first:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_atlas14"
   },
   "outputs": [],
   "source": [
    "# Enter your project coordinates\n",
    "# Example: Minneapolis-St. Paul\n",
    "PROJECT_LAT = 44.9778\n",
    "PROJECT_LON = -93.2650\n",
    "PROJECT_NAME = \"Minneapolis-St. Paul\"\n",
    "\n",
    "# Try to download Atlas 14 data\n",
    "try:\n",
    "    atlas14_data = get_atlas14_data(PROJECT_LAT, PROJECT_LON, units='english')\n",
    "    \n",
    "    print(f\"\\nüìä Precipitation Frequency Data for {PROJECT_NAME}\")\n",
    "    print(f\"   Location: ({PROJECT_LAT:.4f}¬∞N, {PROJECT_LON:.4f}¬∞W)\\n\")\n",
    "    print(atlas14_data)\n",
    "    \n",
    "    print(\"\\nüéØ Common Design Storm Depths (24-hour):\")\n",
    "    print(f\"   10-year:   {atlas14_data.loc['24-hr', '10-yr']:.2f} inches\")\n",
    "    print(f\"   25-year:   {atlas14_data.loc['24-hr', '25-yr']:.2f} inches\")\n",
    "    print(f\"   50-year:   {atlas14_data.loc['24-hr', '50-yr']:.2f} inches\")\n",
    "    print(f\"   100-year:  {atlas14_data.loc['24-hr', '100-yr']:.2f} inches\")\n",
    "    print(f\"   500-year:  {atlas14_data.loc['24-hr', '500-yr']:.2f} inches\")\n",
    "    \n",
    "    API_SUCCESS = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  API access failed\")\n",
    "    API_SUCCESS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "manual_entry"
   },
   "source": [
    "### Manual Data Entry (If API Failed)\n",
    "\n",
    "If the API couldn't connect, manually enter Atlas 14 data:\n",
    "\n",
    "**How to get your data:**\n",
    "1. Visit: https://hdsc.nws.noaa.gov/pfds/\n",
    "2. Click on your project location\n",
    "3. Select \"Precipitation Frequency Data Server (PFDS)\"\n",
    "4. Copy the values from the table\n",
    "5. Paste into the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual_atlas14"
   },
   "outputs": [],
   "source": [
    "# Only run this cell if API failed above\n",
    "\n",
    "if not API_SUCCESS:\n",
    "    # Example: Minneapolis-St. Paul (NOAA Atlas 14 Volume 8)\n",
    "    # Replace with YOUR location's values from NOAA website\n",
    "    \n",
    "    atlas14_data = pd.DataFrame({\n",
    "        '1-yr':   [0.31, 0.42, 0.51, 0.72, 0.96, 1.34, 1.58, 2.07, 2.60, 3.11, 3.48, 3.70, 3.88, 4.24, 4.49, 5.10, 5.57, 6.29, 6.90],\n",
    "        '2-yr':   [0.37, 0.51, 0.62, 0.88, 1.18, 1.65, 1.95, 2.58, 3.25, 3.93, 4.43, 4.73, 4.96, 5.45, 5.79, 6.59, 7.21, 8.15, 8.94],\n",
    "        '5-yr':   [0.45, 0.62, 0.76, 1.09, 1.46, 2.07, 2.45, 3.28, 4.17, 5.09, 5.77, 6.18, 6.50, 7.17, 7.63, 8.71, 9.54, 10.81, 11.87],\n",
    "        '10-yr':  [0.52, 0.71, 0.87, 1.25, 1.69, 2.40, 2.85, 3.86, 4.93, 6.06, 6.91, 7.42, 7.82, 8.66, 9.23, 10.56, 11.58, 13.13, 14.43],\n",
    "        '25-yr':  [0.61, 0.84, 1.03, 1.48, 2.01, 2.88, 3.42, 4.68, 6.01, 7.45, 8.54, 9.19, 9.70, 10.79, 11.52, 13.20, 14.49, 16.45, 18.10],\n",
    "        '50-yr':  [0.68, 0.94, 1.16, 1.67, 2.27, 3.26, 3.88, 5.34, 6.89, 8.57, 9.87, 10.64, 11.24, 12.54, 13.40, 15.39, 16.91, 19.22, 21.16],\n",
    "        '100-yr': [0.76, 1.05, 1.29, 1.87, 2.54, 3.67, 4.38, 6.06, 7.85, 9.80, 11.32, 12.23, 12.93, 14.46, 15.48, 17.80, 19.56, 22.27, 24.52],\n",
    "        '200-yr': [0.84, 1.16, 1.43, 2.07, 2.82, 4.09, 4.89, 6.81, 8.85, 11.09, 12.84, 13.89, 14.70, 16.48, 17.66, 20.34, 22.36, 25.49, 28.09],\n",
    "        '500-yr': [0.95, 1.32, 1.63, 2.36, 3.22, 4.69, 5.61, 7.87, 10.27, 12.93, 15.00, 16.26, 17.23, 19.37, 20.78, 23.97, 26.37, 30.12, 33.22],\n",
    "        '1000-yr':[1.05, 1.46, 1.80, 2.61, 3.57, 5.20, 6.23, 8.75, 11.46, 14.47, 16.82, 18.25, 19.36, 21.80, 23.41, 27.03, 29.74, 34.03, 37.56]\n",
    "    }, index=['5-min', '10-min', '15-min', '30-min', '60-min', \n",
    "              '2-hr', '3-hr', '6-hr', '12-hr', '24-hr', '2-day', '3-day', \n",
    "              '4-day', '7-day', '10-day', '20-day', '30-day', '45-day', '60-day'])\n",
    "    \n",
    "    print(f\"‚úÖ Atlas 14 data loaded manually ({PROJECT_NAME})\")\n",
    "    print(f\"\\nüìä Precipitation Frequency Data:\\n\")\n",
    "    print(atlas14_data)\n",
    "    \n",
    "    print(\"\\nüéØ Common Design Storm Depths (24-hour):\")\n",
    "    print(f\"   10-year:   {atlas14_data.loc['24-hr', '10-yr']:.2f} inches\")\n",
    "    print(f\"   100-year:  {atlas14_data.loc['24-hr', '100-yr']:.2f} inches\")\n",
    "    print(f\"   500-year:  {atlas14_data.loc['24-hr', '500-yr']:.2f} inches\")\n",
    "else:\n",
    "    print(\"‚úÖ Atlas 14 data already loaded from API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "temporal_distribution"
   },
   "source": [
    "## Part 4: Temporal Distribution - MSE 6 24-Hour Pattern üìä\n",
    "\n",
    "### What is a Temporal Distribution?\n",
    "\n",
    "A **temporal distribution** defines how total precipitation is spread over the storm duration:\n",
    "- Atlas 14 gives you **total depth** (e.g., 6.5 inches)\n",
    "- Temporal distribution tells you **when** it falls\n",
    "\n",
    "### Common Distributions\n",
    "\n",
    "- **SCS Type II**: Standard for most of US, peak at ~12 hours\n",
    "- **SCS Type IA**: Pacific maritime climate\n",
    "- **SCS Type III**: Gulf Coast and Florida\n",
    "- **MSE 6**: Minnesota-specific 24-hour distribution\n",
    "\n",
    "### Upload Your Distribution File\n",
    "\n",
    "**Please upload** your `Rainfall_Depth_vs__Time_Table.csv` file using the file upload button below.\n",
    "\n",
    "**Expected format:**\n",
    "- Column 1: Time (hours from start)\n",
    "- Column 2: Cumulative depth (as fraction of total, 0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_distribution"
   },
   "outputs": [],
   "source": [
    "# Upload temporal distribution file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Please upload your temporal distribution CSV file\")\n",
    "print(\"   Expected: Rainfall_Depth_vs__Time_Table.csv\")\n",
    "print(\"   Format: [Time_hours, Cumulative_fraction]\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "filename = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_distribution"
   },
   "outputs": [],
   "source": [
    "# Load and process the temporal distribution\n",
    "temporal_dist = pd.read_csv(filename)\n",
    "\n",
    "# Display the distribution\n",
    "print(\"üìä Temporal Distribution Data:\")\n",
    "print(f\"   Shape: {temporal_dist.shape[0]} time steps\\n\")\n",
    "print(temporal_dist.head(10))\n",
    "\n",
    "# Get column names (first column = time, second = cumulative)\n",
    "time_col = temporal_dist.columns[0]\n",
    "cumul_col = temporal_dist.columns[1]\n",
    "\n",
    "print(f\"\\n‚úÖ Distribution loaded successfully!\")\n",
    "print(f\"   Time column: '{time_col}'\")\n",
    "print(f\"   Cumulative column: '{cumul_col}'\")\n",
    "print(f\"   Duration: {temporal_dist[time_col].max():.1f} hours\")\n",
    "\n",
    "# Rename for easier use\n",
    "temporal_dist = temporal_dist.rename(columns={\n",
    "    time_col: 'time_hr',\n",
    "    cumul_col: 'cumulative_fraction'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_distribution"
   },
   "outputs": [],
   "source": [
    "# Visualize the temporal distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative distribution\n",
    "ax1.plot(temporal_dist['time_hr'], temporal_dist['cumulative_fraction'], \n",
    "         'o-', linewidth=2, markersize=4, color='darkblue')\n",
    "ax1.fill_between(temporal_dist['time_hr'], temporal_dist['cumulative_fraction'], \n",
    "                 alpha=0.3, color='steelblue')\n",
    "ax1.set_xlabel('Time (hours)', fontweight='bold')\n",
    "ax1.set_ylabel('Cumulative Depth (fraction of total)', fontweight='bold')\n",
    "ax1.set_title('MSE 6 Cumulative Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, temporal_dist['time_hr'].max())\n",
    "ax1.set_ylim(0, 1.0)\n",
    "\n",
    "# Calculate incremental (difference between successive cumulative values)\n",
    "incremental = temporal_dist['cumulative_fraction'].diff().fillna(temporal_dist['cumulative_fraction'].iloc[0])\n",
    "\n",
    "# Incremental distribution\n",
    "ax2.bar(temporal_dist['time_hr'], incremental, \n",
    "       width=temporal_dist['time_hr'].diff().median(), \n",
    "       color='darkgreen', alpha=0.7, edgecolor='darkgreen')\n",
    "ax2.set_xlabel('Time (hours)', fontweight='bold')\n",
    "ax2.set_ylabel('Incremental Depth (fraction of total)', fontweight='bold')\n",
    "ax2.set_title('MSE 6 Incremental Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, temporal_dist['time_hr'].max())\n",
    "\n",
    "# Find peak\n",
    "peak_idx = incremental.idxmax()\n",
    "peak_time = temporal_dist.loc[peak_idx, 'time_hr']\n",
    "ax2.annotate('Peak', xy=(peak_time, incremental.iloc[peak_idx]), \n",
    "            xytext=(peak_time+2, incremental.iloc[peak_idx]+0.05),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Distribution characteristics:\")\n",
    "print(f\"   Peak occurs at: {peak_time:.1f} hours ({peak_time/24*100:.0f}% of duration)\")\n",
    "print(f\"   Peak increment: {incremental.iloc[peak_idx]:.3f} (fraction of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyetograph_generation"
   },
   "source": [
    "## Part 5: Design Storm Hyetograph Generation üåßÔ∏è\n",
    "\n",
    "### The Hyetograph Function\n",
    "\n",
    "This function creates a design storm hyetograph by:\n",
    "1. Taking total precipitation depth from Atlas 14\n",
    "2. Applying the temporal distribution\n",
    "3. Interpolating to desired time interval\n",
    "4. Returning a complete time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyetograph_function"
   },
   "outputs": [],
   "source": [
    "def generate_design_storm(total_depth_in, temporal_dist, interval_min=6, start_datetime=None):\n",
    "    \"\"\"\n",
    "    Generate design storm hyetograph from temporal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    total_depth_in : float\n",
    "        Total precipitation depth (inches) from Atlas 14\n",
    "    temporal_dist : pd.DataFrame\n",
    "        Temporal distribution with columns ['time_hr', 'cumulative_fraction']\n",
    "    interval_min : int\n",
    "        Time interval for output (minutes)\n",
    "    start_datetime : str or datetime, optional\n",
    "        Start date/time for the storm (default: '2024-01-01 00:00')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Hyetograph with columns:\n",
    "        - datetime: Timestamp\n",
    "        - time_hr: Time from start (hours)\n",
    "        - incremental_in: Precipitation per interval (inches)\n",
    "        - cumulative_in: Cumulative precipitation (inches)\n",
    "        - intensity_in_hr: Rainfall intensity (inches/hour)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default start time\n",
    "    if start_datetime is None:\n",
    "        start_datetime = pd.Timestamp('2024-01-01 00:00:00')\n",
    "    else:\n",
    "        start_datetime = pd.Timestamp(start_datetime)\n",
    "    \n",
    "    # Get storm duration from temporal distribution\n",
    "    duration_hr = temporal_dist['time_hr'].max()\n",
    "    \n",
    "    # Create output time array at desired interval\n",
    "    num_intervals = int(duration_hr * 60 / interval_min) + 1\n",
    "    output_time_hr = np.linspace(0, duration_hr, num_intervals)\n",
    "    \n",
    "    # Interpolate cumulative distribution to output intervals\n",
    "    cumulative_fraction = np.interp(output_time_hr, \n",
    "                                     temporal_dist['time_hr'], \n",
    "                                     temporal_dist['cumulative_fraction'])\n",
    "    \n",
    "    # Convert to actual depths\n",
    "    cumulative_in = cumulative_fraction * total_depth_in\n",
    "    \n",
    "    # Calculate incremental depths\n",
    "    incremental_in = np.diff(cumulative_in, prepend=0)\n",
    "    \n",
    "    # Calculate intensity (in/hr)\n",
    "    intensity_in_hr = incremental_in / (interval_min / 60)\n",
    "    \n",
    "    # Create datetime index\n",
    "    datetime_index = pd.date_range(start=start_datetime, \n",
    "                                   periods=num_intervals, \n",
    "                                   freq=f'{interval_min}min')\n",
    "    \n",
    "    # Build DataFrame\n",
    "    hyetograph = pd.DataFrame({\n",
    "        'datetime': datetime_index,\n",
    "        'time_hr': output_time_hr,\n",
    "        'incremental_in': incremental_in,\n",
    "        'cumulative_in': cumulative_in,\n",
    "        'intensity_in_hr': intensity_in_hr\n",
    "    })\n",
    "    \n",
    "    return hyetograph\n",
    "\n",
    "print(\"‚úÖ Design storm generator defined!\")\n",
    "print(\"   Ready to create hyetographs from Atlas 14 data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "single_storm_example"
   },
   "source": [
    "### Example: Generate Single Design Storm\n",
    "\n",
    "Let's generate one storm to verify the function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_storm"
   },
   "outputs": [],
   "source": [
    "# Generate 100-year, 24-hour design storm\n",
    "total_depth_100yr = atlas14_data.loc['24-hr', '100-yr']\n",
    "\n",
    "storm_100yr = generate_design_storm(\n",
    "    total_depth_in=total_depth_100yr,\n",
    "    temporal_dist=temporal_dist,\n",
    "    interval_min=6\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated 100-year, 24-hour design storm\")\n",
    "print(f\"   Total depth: {total_depth_100yr:.2f} inches\")\n",
    "print(f\"   Intervals: {len(storm_100yr)}\")\n",
    "print(f\"   Time step: 15 minutes\")\n",
    "print(f\"   Peak intensity: {storm_100yr['intensity_in_hr'].max():.2f} in/hr\")\n",
    "print(f\"\\nüìä Storm hyetograph (first 10 intervals):\\n\")\n",
    "print(storm_100yr[['datetime', 'incremental_in', 'intensity_in_hr']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_example_storm"
   },
   "outputs": [],
   "source": [
    "# Plot the example storm\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Incremental precipitation\n",
    "ax1.bar(storm_100yr['time_hr'], storm_100yr['incremental_in'], \n",
    "       width=0.2, color='darkblue', alpha=0.7)\n",
    "ax1.set_ylabel('Incremental Precipitation (inches)', fontweight='bold')\n",
    "ax1.set_title(f'100-Year, 24-Hour Design Storm - {PROJECT_NAME}\\nTotal: {total_depth_100yr:.2f} inches', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 24)\n",
    "\n",
    "# Mark peak\n",
    "peak_idx = storm_100yr['incremental_in'].idxmax()\n",
    "peak_time = storm_100yr.loc[peak_idx, 'time_hr']\n",
    "peak_value = storm_100yr.loc[peak_idx, 'incremental_in']\n",
    "ax1.annotate(f'Peak: {storm_100yr.loc[peak_idx, \"intensity_in_hr\"]:.2f} in/hr', \n",
    "            xy=(peak_time, peak_value),\n",
    "            xytext=(peak_time+2, peak_value+0.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "# Cumulative precipitation\n",
    "ax2.plot(storm_100yr['time_hr'], storm_100yr['cumulative_in'], \n",
    "        color='darkblue', linewidth=2.5)\n",
    "ax2.fill_between(storm_100yr['time_hr'], storm_100yr['cumulative_in'], \n",
    "                alpha=0.3, color='steelblue')\n",
    "ax2.set_xlabel('Time (hours)', fontweight='bold', fontsize=11)\n",
    "ax2.set_ylabel('Cumulative Precipitation (inches)', fontweight='bold')\n",
    "ax2.set_title('Cumulative Rainfall Curve', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_processing"
   },
   "source": [
    "## Part 6: Batch Processing Multiple AEP Events üîÑ\n",
    "\n",
    "### The Workflow\n",
    "\n",
    "Real H&H projects require multiple design storms:\n",
    "- **Flood frequency analysis**: 10-yr, 50-yr, 100-yr, 500-yr\n",
    "- **Risk assessment**: Range of probabilities\n",
    "- **Design optimization**: Compare alternatives\n",
    "\n",
    "### Batch Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_function"
   },
   "outputs": [],
   "source": [
    "def generate_aep_suite(atlas14_data, temporal_dist, aep_list, duration='24-hr', \n",
    "                       interval_min=6, output_dir=None):\n",
    "    \"\"\"\n",
    "    Generate design storms for multiple AEP events.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    atlas14_data : pd.DataFrame\n",
    "        Atlas 14 precipitation frequency data\n",
    "    temporal_dist : pd.DataFrame\n",
    "        Temporal distribution pattern\n",
    "    aep_list : list of int\n",
    "        AEP values (return periods in years), e.g., [10, 25, 50, 100, 500]\n",
    "    duration : str\n",
    "        Storm duration (must match Atlas 14 index), e.g., '24-hr'\n",
    "    interval_min : int\n",
    "        Output time interval (minutes)\n",
    "    output_dir : str or Path, optional\n",
    "        Directory to save CSV files (created if doesn't exist)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with AEP as keys, hyetographs as values\n",
    "    pd.DataFrame : Summary table of storm characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Generating design storm suite...\")\n",
    "    print(f\"   Duration: {duration}\")\n",
    "    print(f\"   AEP Events: {aep_list}\")\n",
    "    print(f\"   Time interval: {interval_min} minutes\\n\")\n",
    "    \n",
    "    storms = {}\n",
    "    summary_data = []\n",
    "    \n",
    "    # Create output directory if specified\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"üìÅ Output directory: {output_dir}\\n\")\n",
    "    \n",
    "    for aep in aep_list:\n",
    "        print(f\"   Generating {aep}-year storm...\")\n",
    "        \n",
    "        # Get total depth\n",
    "        col_name = f'{aep}-yr'\n",
    "        total_depth = atlas14_data.loc[duration, col_name]\n",
    "        \n",
    "        # Generate hyetograph\n",
    "        hyeto = generate_design_storm(\n",
    "            total_depth_in=total_depth,\n",
    "            temporal_dist=temporal_dist,\n",
    "            interval_min=interval_min\n",
    "        )\n",
    "        \n",
    "        storms[aep] = hyeto\n",
    "        \n",
    "        # Calculate statistics\n",
    "        summary_data.append({\n",
    "            'AEP': f'{aep}-year',\n",
    "            'Probability': f'{100/aep:.2f}%',\n",
    "            'Total_Depth_in': total_depth,\n",
    "            'Peak_Intensity_in_hr': hyeto['intensity_in_hr'].max(),\n",
    "            'Peak_Time_hr': hyeto.loc[hyeto['intensity_in_hr'].idxmax(), 'time_hr'],\n",
    "            'Intervals': len(hyeto)\n",
    "        })\n",
    "        \n",
    "        # Save to CSV if output directory specified\n",
    "        if output_dir is not None:\n",
    "            filename = output_dir / f'Storm_{aep}yr_{duration}.csv'\n",
    "            hyeto.to_csv(filename, index=False)\n",
    "            print(f\"      Saved: {filename.name}\")\n",
    "    \n",
    "    # Create summary table\n",
    "    summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(aep_list)} design storms successfully!\\n\")\n",
    "    \n",
    "    return storms, summary\n",
    "\n",
    "print(\"‚úÖ Batch processing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_batch"
   },
   "source": [
    "### Generate Your Design Storm Suite\n",
    "\n",
    "Customize the AEP list for your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_suite"
   },
   "outputs": [],
   "source": [
    "# Define your AEP events\n",
    "AEP_EVENTS = [10, 25, 50, 100, 500]  # Modify as needed\n",
    "\n",
    "# Generate all storms\n",
    "design_storms, storm_summary = generate_aep_suite(\n",
    "    atlas14_data=atlas14_data,\n",
    "    temporal_dist=temporal_dist,\n",
    "    aep_list=AEP_EVENTS,\n",
    "    duration='24-hr',\n",
    "    interval_min=6,\n",
    "    output_dir='Design_Storms'  # Creates folder and saves CSVs\n",
    ")\n",
    "\n",
    "print(\"\\nüìä STORM SUMMARY TABLE\\n\")\n",
    "print(storm_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison_plots"
   },
   "source": [
    "### Comparative Visualization\n",
    "\n",
    "Plot all storms together for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_comparison"
   },
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Define colors for each storm\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(AEP_EVENTS)))\n",
    "\n",
    "# Plot incremental precipitation\n",
    "for i, aep in enumerate(AEP_EVENTS):\n",
    "    storm = design_storms[aep]\n",
    "    ax1.plot(storm['time_hr'], storm['incremental_in'], \n",
    "            label=f'{aep}-year', linewidth=2, color=colors[i], alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Incremental Precipitation (inches)', fontweight='bold', fontsize=11)\n",
    "ax1.set_title(f'Design Storm Comparison - {PROJECT_NAME}\\n24-Hour Duration | MSE 6 Distribution', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=10, framealpha=0.9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 24)\n",
    "\n",
    "# Plot cumulative precipitation\n",
    "for i, aep in enumerate(AEP_EVENTS):\n",
    "    storm = design_storms[aep]\n",
    "    ax2.plot(storm['time_hr'], storm['cumulative_in'], \n",
    "            label=f'{aep}-year: {storm[\"cumulative_in\"].iloc[-1]:.2f} in', \n",
    "            linewidth=2.5, color=colors[i])\n",
    "\n",
    "ax2.set_xlabel('Time (hours)', fontweight='bold', fontsize=11)\n",
    "ax2.set_ylabel('Cumulative Precipitation (inches)', fontweight='bold', fontsize=11)\n",
    "ax2.set_title('Cumulative Rainfall Curves', fontsize=13, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=10, framealpha=0.9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Design_Storm_Comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison plot saved: Design_Storm_Comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_tables"
   },
   "source": [
    "## Part 7: Professional Summary Tables üìã\n",
    "\n",
    "### Create Report-Ready Tables\n",
    "\n",
    "Generate tables suitable for engineering reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_summary"
   },
   "outputs": [],
   "source": [
    "# Enhanced summary table with additional statistics\n",
    "def create_detailed_summary(storms_dict, atlas14_data, duration='24-hr'):\n",
    "    \"\"\"\n",
    "    Create comprehensive summary table for engineering reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_rows = []\n",
    "    \n",
    "    for aep, storm in storms_dict.items():\n",
    "        # Get Atlas 14 depth\n",
    "        total_depth = atlas14_data.loc[duration, f'{aep}-yr']\n",
    "        \n",
    "        # Find peak\n",
    "        peak_idx = storm['intensity_in_hr'].idxmax()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        row = {\n",
    "            'Return Period': f'{aep}-year',\n",
    "            'AEP (%)': f'{100/aep:.2f}',\n",
    "            'Total Depth (in)': f'{total_depth:.2f}',\n",
    "            'Peak Intensity (in/hr)': f'{storm.loc[peak_idx, \"intensity_in_hr\"]:.2f}',\n",
    "            'Peak Time (hr)': f'{storm.loc[peak_idx, \"time_hr\"]:.1f}',\n",
    "            'Duration (hr)': f'{storm[\"time_hr\"].max():.1f}',\n",
    "            'Time Steps': len(storm),\n",
    "            'Interval (min)': int((storm['time_hr'].iloc[1] - storm['time_hr'].iloc[0]) * 60)\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(summary_rows)\n",
    "\n",
    "# Create detailed summary\n",
    "detailed_summary = create_detailed_summary(design_storms, atlas14_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"DESIGN STORM SUMMARY - {PROJECT_NAME}\")\n",
    "print(f\"Location: {PROJECT_LAT:.4f}¬∞N, {PROJECT_LON:.4f}¬∞W\")\n",
    "print(f\"Temporal Distribution: MSE 6 24-Hour\")\n",
    "print(f\"Date Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n\" + detailed_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "\n",
    "# Save to CSV\n",
    "detailed_summary.to_csv('Design_Storm_Summary.csv', index=False)\n",
    "print(\"\\n‚úÖ Summary table saved: Design_Storm_Summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intensity_table"
   },
   "source": [
    "### Peak Intensity Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "intensity_comparison"
   },
   "outputs": [],
   "source": [
    "# Create peak intensity comparison\n",
    "intensity_data = []\n",
    "\n",
    "for aep, storm in design_storms.items():\n",
    "    peak_idx = storm['intensity_in_hr'].idxmax()\n",
    "    \n",
    "    # Get peak 15-min, 1-hr, and 3-hr intensities\n",
    "    peak_15min = storm.loc[peak_idx, 'incremental_in']\n",
    "    peak_intensity = storm.loc[peak_idx, 'intensity_in_hr']\n",
    "    \n",
    "    # Calculate 1-hour total (10 consecutive 6-min intervals = 60 minutes)\n",
    "    if peak_idx >= 5:\n",
    "        one_hr_total = storm.loc[peak_idx-5:peak_idx+4, 'incremental_in'].sum()\n",
    "    else:\n",
    "        one_hr_total = storm.loc[:10, 'incremental_in'].sum()\n",
    "    \n",
    "    intensity_data.append({\n",
    "        'Return Period': f'{aep}-year',\n",
    "        'Peak 6-min (in)': f'{peak_15min:.3f}',\n",
    "        'Peak Intensity (in/hr)': f'{peak_intensity:.2f}',\n",
    "        'Max 1-hr Total (in)': f'{one_hr_total:.2f}'\n",
    "    })\n",
    "\n",
    "intensity_table = pd.DataFrame(intensity_data)\n",
    "\n",
    "print(\"\\nüìä PEAK INTENSITY COMPARISON\\n\")\n",
    "print(intensity_table.to_string(index=False))\n",
    "\n",
    "intensity_table.to_csv('Peak_Intensity_Summary.csv', index=False)\n",
    "print(\"\\n‚úÖ Intensity table saved: Peak_Intensity_Summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_formats"
   },
   "source": [
    "## Part 8: Export for H&H Models üíæ\n",
    "\n",
    "### HEC-HMS Format\n",
    "\n",
    "Export in formats compatible with HEC-HMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_hms"
   },
   "outputs": [],
   "source": [
    "# Create HEC-HMS compatible exports\n",
    "import zipfile\n",
    "\n",
    "def export_for_hms(storms_dict, output_dir='HEC_HMS_Export'):\n",
    "    \"\"\"\n",
    "    Export design storms in HEC-HMS compatible format.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üì§ Exporting for HEC-HMS...\\n\")\n",
    "    \n",
    "    for aep, storm in storms_dict.items():\n",
    "        # Create HMS format (DateTime, Precipitation)\n",
    "        hms_data = storm[['datetime', 'incremental_in']].copy()\n",
    "        hms_data.columns = ['DateTime', 'Precipitation_in']\n",
    "        \n",
    "        # Save\n",
    "        filename = output_path / f'Precip_{aep}yr_HMS.csv'\n",
    "        hms_data.to_csv(filename, index=False)\n",
    "        print(f\"   ‚úÖ {filename.name}\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_path = output_path / 'README.txt'\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(f\"HEC-HMS Precipitation Data\\n\")\n",
    "        f.write(f\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Project: {PROJECT_NAME}\\n\")\n",
    "        f.write(f\"Location: {PROJECT_LAT:.4f}¬∞N, {PROJECT_LON:.4f}¬∞W\\n\")\n",
    "        f.write(f\"Source: NOAA Atlas 14\\n\")\n",
    "        f.write(f\"Temporal Distribution: MSE 6 24-Hour\\n\")\n",
    "        f.write(f\"Time Interval: 15 minutes\\n\")\n",
    "        f.write(f\"Duration: 24 hours\\n\\n\")\n",
    "        f.write(f\"Files:\\n\")\n",
    "        for aep in storms_dict.keys():\n",
    "            f.write(f\"  - Precip_{aep}yr_HMS.csv\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ HEC-HMS export complete: {output_dir}/\")\n",
    "\n",
    "# Export all storms\n",
    "export_for_hms(design_storms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_all"
   },
   "source": [
    "### Download All Files\n",
    "\n",
    "Create a ZIP file with all outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_zip"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive ZIP file\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def create_project_archive(project_name):\n",
    "    \"\"\"\n",
    "    Create ZIP archive with all project outputs.\n",
    "    \"\"\"\n",
    "    zip_filename = f'{project_name.replace(\" \", \"_\")}_Design_Storms.zip'\n",
    "    \n",
    "    print(f\"üì¶ Creating project archive...\\n\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add design storm CSVs\n",
    "        for file in Path('Design_Storms').glob('*.csv'):\n",
    "            zipf.write(file, f'Design_Storms/{file.name}')\n",
    "            print(f\"   Added: {file.name}\")\n",
    "        \n",
    "        # Add HEC-HMS exports\n",
    "        for file in Path('HEC_HMS_Export').glob('*'):\n",
    "            zipf.write(file, f'HEC_HMS_Export/{file.name}')\n",
    "            print(f\"   Added: {file.name}\")\n",
    "        \n",
    "        # Add summary tables\n",
    "        summary_files = ['Design_Storm_Summary.csv', 'Peak_Intensity_Summary.csv']\n",
    "        for file in summary_files:\n",
    "            if Path(file).exists():\n",
    "                zipf.write(file, f'Summary_Tables/{file}')\n",
    "                print(f\"   Added: {file}\")\n",
    "        \n",
    "        # Add comparison plot\n",
    "        if Path('Design_Storm_Comparison.png').exists():\n",
    "            zipf.write('Design_Storm_Comparison.png', 'Figures/Design_Storm_Comparison.png')\n",
    "            print(f\"   Added: Design_Storm_Comparison.png\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Archive created: {zip_filename}\")\n",
    "    return zip_filename\n",
    "\n",
    "# Create archive\n",
    "archive_file = create_project_archive(PROJECT_NAME)\n",
    "\n",
    "# Download in Colab\n",
    "print(f\"\\nüì• Downloading archive...\")\n",
    "files.download(archive_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## üéâ What You Can Now Do!\n",
    "\n",
    "Congratulations! You've completed Module 6 and mastered precipitation frequency analysis.\n",
    "\n",
    "### ‚úÖ You Can Now:\n",
    "\n",
    "**Data Access:**\n",
    "- Access NOAA Atlas 14 precipitation frequency data via API\n",
    "- Handle connection errors and use manual data entry\n",
    "- Understand precipitation frequency concepts (ARI, AEP)\n",
    "\n",
    "**Design Storm Generation:**\n",
    "- Upload and apply custom temporal distributions\n",
    "- Generate design storm hyetographs at any time interval\n",
    "- Create multiple AEP events in a single batch\n",
    "\n",
    "**Professional Outputs:**\n",
    "- Build comprehensive summary tables\n",
    "- Create comparative visualizations\n",
    "- Export data for HEC-HMS and HEC-RAS\n",
    "- Package everything for project delivery\n",
    "\n",
    "**Automation:**\n",
    "- Batch process multiple return periods\n",
    "- Generate consistent outputs across projects\n",
    "- Create reproducible workflows\n",
    "\n",
    "### üöÄ Real-World Applications:\n",
    "\n",
    "You can now:\n",
    "- Generate complete design storm suites for flood studies\n",
    "- Support FEMA floodplain analyses\n",
    "- Prepare precipitation inputs for H&H models\n",
    "- Create professional engineering reports\n",
    "- Automate repetitive precipitation analysis tasks\n",
    "\n",
    "### üìö Key Functions You've Learned:\n",
    "\n",
    "```python\n",
    "# Get Atlas 14 data\n",
    "atlas14_data = get_atlas14_data(lat, lon)\n",
    "\n",
    "# Generate single storm\n",
    "storm = generate_design_storm(total_depth, temporal_dist)\n",
    "\n",
    "# Batch process multiple AEPs\n",
    "storms, summary = generate_aep_suite(atlas14_data, temporal_dist, [10, 50, 100])\n",
    "\n",
    "# Export for HMS\n",
    "export_for_hms(storms)\n",
    "```\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "1. **Always document** your Atlas 14 source and date\n",
    "2. **Verify** temporal distribution is appropriate for your region\n",
    "3. **Check** time intervals match model requirements\n",
    "4. **Create** summary tables for every project\n",
    "5. **Archive** all inputs and outputs together\n",
    "\n",
    "### üîó Next Steps:\n",
    "\n",
    "- Apply to your own projects\n",
    "- Customize temporal distributions\n",
    "- Integrate with HEC-HMS/RAS workflows\n",
    "- Build project-specific templates\n",
    "\n",
    "**You're now ready to handle precipitation frequency analysis professionally!** ‚òîüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_reference"
   },
   "source": [
    "## Appendix: Quick Reference üìã\n",
    "\n",
    "### Common AEP/ARI Conversions\n",
    "\n",
    "| AEP | ARI | Use Case |\n",
    "|-----|-----|----------|\n",
    "| 50% | 2-yr | Frequent flooding |\n",
    "| 20% | 5-yr | Nuisance flooding |\n",
    "| 10% | 10-yr | Minor flooding, drainage design |\n",
    "| 4% | 25-yr | Moderate flooding |\n",
    "| 2% | 50-yr | Major flooding |\n",
    "| 1% | 100-yr | **Base flood elevation (FEMA)** |\n",
    "| 0.5% | 200-yr | Freeboard |\n",
    "| 0.2% | 500-yr | Critical infrastructure |\n",
    "\n",
    "### Quick Workflow\n",
    "\n",
    "```python\n",
    "# 1. Get Atlas 14 data\n",
    "atlas14_data = get_atlas14_data(lat, lon)\n",
    "\n",
    "# 2. Upload temporal distribution\n",
    "temporal_dist = pd.read_csv('distribution.csv')\n",
    "\n",
    "# 3. Generate storms\n",
    "storms, summary = generate_aep_suite(\n",
    "    atlas14_data, temporal_dist, [10, 50, 100]\n",
    ")\n",
    "\n",
    "# 4. Export\n",
    "export_for_hms(storms)\n",
    "```\n",
    "\n",
    "### Typical Time Intervals\n",
    "\n",
    "- **6 minutes (0.1 hr)**: High-resolution urban hydrology, detailed storm analysis\n- **15 minutes**: Standard urban hydrology\n",
    "- **1 hour**: Most common for H&H modeling\n",
    "- **6 hours**: Large watersheds, simplified analysis\n",
    "- **1 day**: Long-duration events\n",
    "\n",
    "### File Outputs\n",
    "\n",
    "```\n",
    "Project/\n",
    "‚îú‚îÄ‚îÄ Design_Storms/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Storm_10yr_24-hr.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Storm_50yr_24-hr.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Storm_100yr_24-hr.csv\n",
    "‚îú‚îÄ‚îÄ HEC_HMS_Export/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Precip_10yr_HMS.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ README.txt\n",
    "‚îú‚îÄ‚îÄ Summary_Tables/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Design_Storm_Summary.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Peak_Intensity_Summary.csv\n",
    "‚îî‚îÄ‚îÄ Figures/\n",
    "    ‚îî‚îÄ‚îÄ Design_Storm_Comparison.png\n",
    "```\n",
    "\n",
    "Keep this handy for quick reference!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Module6_PrecipitationData_Atlas14.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
